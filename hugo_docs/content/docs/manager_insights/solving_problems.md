---
title: "Solving Problems"
date: 2025-08-14
draft: false
---

### Part 1: The Big Picture - What Problem is this Code Solving?

Imagine you're running a very popular chatbot service. You'll face a few key challenges:

1.  **Overload:** If 1,000 users ask questions at the exact same time, your server might crash. You need a way to manage the load.
2.  **The Thundering Herd:** What if 100 of those users all ask the *exact same question* at the same time (e.g., "What was the score of the big game last night?")? It would be incredibly wasteful to call the expensive AI model 100 times with the same question. It would be much smarter to call it *once* and send the answer to all 100 users.
3.  **Responsiveness:** Users don't want to wait for the full answer to be generated. They want to see the words appear as they are generated by the AI, like in the ChatGPT interface. This is called "streaming."
4.  **User Control:** What if a user gets impatient or realizes they asked the wrong question? They need a way to cancel the request.
5.  **Robustness:** What if a request gets stuck for some reason? You can't let it sit there forever, consuming resources. You need a system to clean up old, broken requests.

The `Manager` is the brain that solves all of these problems. It's the central orchestrator for every chatbot request that comes into the system.

---

### Part 2: The Core Components (The "Cast of Characters")

Let's meet the main `structs` that work together. Think of them as different specialists in a busy office.

#### 1. The `Manager`

This is the boss. It's the main struct that holds everything together. Its fields tell you what it's responsible for:

*   `requestQueue chan ...`: A **waiting line (queue)** for incoming requests. When a new question comes in, it's put in this line. This prevents the system from being overwhelmed.
*   `activeRequests map[...]`: A **directory or filing cabinet**. It keeps track of every single request that is currently in the system, whether it's waiting in the queue, being processed, or has been cancelled.
*   `prepareSemaphore chan struct{}`: A **set of "busy" signs**. This limits how many requests can be *prepared* at the same time. If you have 10 busy signs, the 11th request has to wait.
*   `llmStreamSemaphore chan struct{}`: Another set of **"busy" signs**, but specifically for talking to the expensive AI (the LLM). You might have a stricter limit here because AI calls cost money and resources.
*   `sfGroup *singleflight.Group`: **The "Thundering Herd" tamer**. This is a special tool to solve problem #2. We'll dive deep into this later, but for now, know its job is to ensure that identical requests are only processed once.
*   `broadcasters map[...]`: A **directory of ongoing "press conferences"**. When multiple users ask the same question, one "broadcast" is created, and this map tracks it.
*   `janitor`: A background process that **cleans up old, stuck requests**.

#### 2. The `StreamBroadcaster`

This is a specialized helper. Its job is simple but crucial: **Take one stream of events and fan it out to many listeners.**

Imagine a single TV antenna receiving a signal. The `StreamBroadcaster` is the device that splits this signal and sends it to every TV in an apartment building.

*   `subscribers []chan ...`: The list of all "TVs" (clients/users) that want to receive the stream.
*   `Broadcast(event)`: The main action. It gets a piece of data (one `StreamEvent`) and sends a copy to every subscriber.
*   `Subscribe(sub)`: A new user (a new TV) tunes in. They immediately get all the "history" (the part of the stream they missed) and are then added to the list for live updates.
*   `Close()`: The broadcast is over. It tells all subscribers that there's no more data by closing their channels.

#### 3. `types.RequestStream` (The "Request File Folder")

This is a small struct that acts like a file folder for a *single* user's request. It holds all the important information about that one request.

*   `Stream chan (<-chan ...)`: A channel to deliver the final stream *to the client*.
*   `Err chan error`: A channel to deliver an error if something goes wrong.
*   `ClientConnected chan struct{}`: A signal. This is a clever trick. It's a channel that the `Manager` uses to know when the user's browser has *actually* connected and is ready to receive the stream.
*   `State types.State...`: The current status of the request (e.g., `StateQueued`, `StateProcessing`, `StateCancelled`).

---

### Part 3: The Lifecycle of a Request (Step-by-Step)

Let's follow a question from a user named "Alice" from start to finish.

#### Step 1: `SubmitRequest` - Alice Asks a Question

1.  Alice sends her question: `"What is Go?"`.
2.  `SubmitRequest` is called. It creates a unique `requestID`.
3.  It creates a new `RequestStream` "file folder" for Alice's request and puts it in the `m.activeRequests` map. The state is set to `StateQueued`.
4.  It tries to put Alice's request into the `m.requestQueue`.
    *   If the queue is full, it gives up and returns an error ("queue is full").
    *   If successful, it returns the `requestID` to Alice. She will use this ID to pick up her answer.

#### Step 2: `requestWorkerPool` - A Worker Becomes Available

1.  The `Manager` has a pool of background "workers" (goroutines) constantly watching the `requestQueue`.
2.  One worker sees Alice's request and picks it up.
3.  The worker first has to pass through the `prepareSemaphore`. This is like taking one of the "busy" signs. If all are taken, the worker waits. This controls concurrency.
4.  Once it has a "busy" sign, it calls `processAndRouteRequest`.

#### Step 3: `processAndRouteRequest` - The Critical Junction

This is the most complex part.

1.  **Create a Cache Key:** The code looks at Alice's question and her previous conversation history. It creates a unique string out of it, like `"ai_chatbot_conv:What is Go?"`. This key represents the *exact* question being asked.
2.  **Use `singleflight.Group`:** The worker now uses `m.sfGroup.Do(cacheKey, ...)`
    *   **Scenario A: Alice is the FIRST to ask this question.**
        *   The `singleflight` group has never seen this `cacheKey` before. It says, "You are the **LEADER**."
        *   The code inside the `Do` function is executed.
        *   It creates a new `StreamBroadcaster`.
        *   It starts a new goroutine, `initiateAndManageBroadcast`, to do the actual work of calling the AI.
        *   It returns a `broadcastInfo` struct, which contains the new broadcaster. The `shared` boolean returned by `Do` will be `false`.
    *   **Scenario B: Bob asks "What is Go?" a millisecond after Alice.**
        *   Bob's request goes through the same steps. When it gets to `sfGroup.Do`, the group sees the *exact same* `cacheKey`.
        *   The `singleflight` group says, "Hold on! Someone is already working on this. I'll make you wait, and when they are done, I'll give you their result."
        *   Bob's request is now a **FOLLOWER**. It does *not* execute the function. It waits.
        *   Once Alice's "leader" request has created the `broadcastInfo`, `singleflight` gives that *same* `broadcastInfo` object to Bob's request. The `shared` boolean will be `true`.

3.  **Subscribing:**
    *   Whether a LEADER or a FOLLOWER, the request now has a `broadcastInfo` object.
    *   It calls `info.broadcaster.Subscribe(clientChan)`, effectively "tuning in" to the broadcast for this question.
    *   It then puts the `clientChan` into Alice's (or Bob's) `RequestStream` "file folder" so it can be picked up later.

#### Step 4: `initiateAndManageBroadcast` - The Leader Does the Work

This function is only ever run by the **LEADER** request.

1.  **Wait for the Client to Connect**: This is the **CRITICAL FIX** mentioned in your code. The function immediately waits on `<-streamHolder.ClientConnected`. Why? To save money and resources. There's no point in calling the expensive AI if Alice's browser disconnects before she even tries to read the answer. The process pauses here.
2.  **(Meanwhile) `GetRequestResultStream` - Alice Connects to Get Her Answer:**
    *   Alice's client (e.g., her web browser) calls this function with her `requestID`.
    *   The function finds her `RequestStream` "file folder".
    *   It **closes** the `ClientConnected` channel: `close(streamHolder.ClientConnected)`. This is the signal!
3.  **The Leader Continues:**
    *   Back in `initiateAndManageBroadcast`, the `<-streamHolder.ClientConnected` operation unblocks because the channel was closed. The leader now knows the client is ready.
    *   It acquires a slot from the `llmStreamSemaphore` (takes a "busy" sign for the AI).
    *   It calls `m.streamer.Stream(...)`, which is the function that actually calls the AI model.
    *   The streamer starts getting back chunks of the answer (`"Go", " is", " a", " programming", " language..."`).
    *   For each chunk, it calls `info.broadcaster.Broadcast(event)`.
    *   The broadcaster then sends that chunk to *all* subscribers (Alice and Bob).
4.  **Cleanup:** When the stream is finished, the `defer` block in `initiateAndManageBroadcast` runs. It closes the broadcaster and cleans up the `activeRequests` entries for the leader and all of its followers.

---

### Part 4: Handling Edge Cases - Cancellation and Timeouts

This is where the code's robustness comes in.

#### The `janitor`

*   This is a simple background process that runs on a timer (e.g., every minute).
*   It scans the `m.activeRequests` map.
*   If it finds a request that's been in the `StateQueued` for too long, it cancels and removes it.
*   If it finds a request that's been in `StateProcessing` for too long, it cancels and removes it.
*   This prevents "zombie" requests from clogging the system forever.

#### `CancelStream` - The Tricky Logic

This is where things get interesting, especially with the Leader/Follower dynamic.

1.  A user clicks "cancel". The `CancelStream` function is called with their `requestID`.
2.  It finds the request in `activeRequests` and marks its state as `StateCancelled`.
3.  Now, it has to decide what to do based on the request's role:

    *   **Case 1: The request being cancelled is a FOLLOWER.**
        *   This is easy. It just unsubscribes the follower from the broadcast. The broadcast continues for the Leader and any other followers.
        *   It sends a final "you cancelled this" message to the follower's client.

    *   **Case 2: The request being cancelled is the LEADER, and it has NO followers.**
        *   This is also easy. Since it's the only one interested, the whole operation can be shut down.
        *   It calls the `cancelFunc()` for the underlying context, which tells the `initiateAndManageBroadcast` goroutine to stop everything. The entire broadcast is terminated.

    *   **Case 3: The request being cancelled is the LEADER, but it HAS followers.**
        *   This is the cleverest part. We can't just kill the broadcast, because Bob and other followers still want the answer!
        *   This is called a **"Deceptive Cancellation."**
        *   The leader simply unsubscribes itself from its own broadcast.
        *   It sends a "you cancelled this" message to the leader's client.
        *   **Crucially, it does NOT cancel the underlying process.** The `initiateAndManageBroadcast` goroutine keeps running, generating the answer for all the remaining followers. The show must go on!

This detailed logic ensures that the system is efficient (by not cancelling work others need) and provides correct feedback to every user, no matter their role in the "thundering herd."
